# -*- coding: utf-8 -*-
"""Airline customer segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ofzCbS13IyEwYRqi8hk5Nj2uEuLsWujU

Importing pandas
"""

import pandas as pd

data = pd.read_csv('flight.csv')

data.shape

data.head()

data.info()

"""Finding null values and fill the null values"""

data.isnull().sum()

data['WORK_CITY'].notnull() & data['WORK_PROVINCE'].notnull()

data = data[data['WORK_CITY'].notnull() & data['WORK_PROVINCE'].notnull()]

data.shape

#null values are removed

data.isnull().sum()

data['AGE'].unique()

data['AGE'] = data['AGE'].fillna(data['AGE'].median())

data['AGE'].unique()

data.isnull().sum()

data['SUM_YR_1'].interpolate(method = 'linear',inplace = True)

data.isnull().sum()

data['SUM_YR_2'].interpolate(method = 'linear',inplace = True)
#Interpolate is used to fill null values

data.isnull().sum()

data.nunique().sort_values()

data['GENDER'].value_counts()

data['WORK_COUNTRY'].unique()

data['WORK_COUNTRY'].value_counts()

list1 = ['CN','KR','US','HK','JP']

data = data.loc[data.WORK_COUNTRY.isin(list1)]

data['WORK_COUNTRY'].value_counts()

data.head()

data.drop(columns = ['MEMBER_NO','FFP_DATE','FIRST_FLIGHT_DATE','LAST_FLIGHT_DATE','WORK_CITY','LOAD_TIME'],inplace = True)
#Dropped columns which are not required

data.info()

data = data.reset_index(drop = True)

data.isnull().sum()

data['WORK_COUNTRY'].nunique()

data = pd.get_dummies(data, columns = ['WORK_COUNTRY'])
# Dummies convert categorical variables into dummy or indicator variables

data.info()

data['WORK_PROVINCE'].nunique()

data = pd.get_dummies(data, columns = ['WORK_PROVINCE'])

data.shape

data.head()

data = pd.get_dummies(data, columns = ['GENDER'])

data.info()

data.dtypes

data.info(verbose = True, show_counts = True)

"""Preprocessing the data"""

from sklearn.preprocessing import StandardScaler

SS = StandardScaler()

data = SS.fit_transform (data)

from sklearn.cluster import KMeans

km = KMeans()

y_km = km.fit_predict(data)

y_km[0:10]

inertias = []

for k in range (1,11):
  km = KMeans(n_clusters = k, random_state = 10)
  km.fit(data)
  inertias.append(km.inertia_)

inertias

"""Visualize the data"""

import matplotlib.pyplot as plt

plt.plot(range(1,11), inertias, marker = 'o')
plt.xlabel('k')
plt.ylabel('obj')

#Above graph shows that data don't have good clustering so elbow not formed

from sklearn.metrics import silhouette_score

# Silhouette score is the measure of how similar a data point is within the cluster compared to other clusters

silhouette_scores = []
inertias = []

for k in range(2,11):
  km = KMeans(n_clusters = k, random_state = 10)
  y_km = km.fit_predict(data)
  inertias.append(km.inertia_)
  silhouette_scores.append(silhouette_score(data, y_km))

silhouette_scores

# Best value : 0.16624
# Worst value: -0.097